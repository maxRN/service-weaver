\documentclass[sigconf,review,9pt]{acmart}
\graphicspath{ {./figures/} }
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{multirow}

\title{Service Weaver: A new era of developing microservice architectures?}
\author{Max Gro√üe}
\date{June 2023}

\begin{document}

\begin{abstract}
	Microservice architecture are quickly becoming the default architecture style
	in the industry, increasing development velocity and reducing scaling issues.
	However numerous problems have been identified that plague microservice development,
	such as designing the overall architecture and high network usage.
	Service Weaver is a new framework that aims to solve some of the challenges involved
	in microservice development.
	In this paper I find that Service Weaver shows up to a 10\% reduction in message size
	and thus faster communication speeds.
	It also simplifies operating large applications by providing integration with
	logging, metrics, and trace tooling of cloud providers.
	It remains to be seen whether the simplified configuration of Service Weaver application
	can cover all use cases.
\end{abstract}

\maketitle

\section{Introduction}
Microservices are the predominant way to build modern applications in the industry. \cite{wang_ms_current_situation}
The advantages over the typical monolithic development have been numerous:
better horizontal scalability due to individually deployable and scalable entities,
better suited for large teams due to independence of each other's implementation,
and more agile development.

However, recent papers have shown that microservice architectures bring their own share
of problems that developers have to deal with.
In \cite{soldani_pains_gains} the authors analyze the pains for MSA development
and create a taxonomy for each stage of the lifecycle of software (design, development, and operation).
Architecture design, testing during development, and monitoring and resource consumption
while operating the application are among the most often mentioned topics.

Service Weaver is a new development framework created by Google that aims to solve some of
the problems with current microservice development.
Their solution offers the flexibility to write the application as a typical monolith
but deploy it as a set of microservices, thus eliminating the problem of pre-mature
splitting of the application.

To investigate whether Service Weaver can be a feasible alternative to conventional
MSA development I devised multiple experiments to empirically determine the performance
claims of Service Weaver.
I will also try to present how easy or complicated certain operations during the
lifecycle of a typical software application are for the two applications.

\subsection{Related Work}
The team behind Service Weaver present the programming model and architecture
as well as a few performance measurements in \cite{service_weaver_paper}.
While they go in-depth on the technical details of the architecture, they don't
specify how exactly the tests were performed.

\Citeauthor{soldani_pains_gains} conduct a grey literature review of pains and gains
associated with microservice development.
They also present a pain taxonomy of problems related to microservice application
design, development, and operation.
They don't however present any solution to relieve these problems.

\section{Fundamentals}

To effectively measure the differences in performance and to understand the results
it is important to understand the different architectures and technical details.

\subsection{Service Weaver}
Service Weaver is a new framework developed by Google for creating Microservice
Architectures in Go. \cite{serviceWeaverSite}
It enables the developer to write their app in a monolithic fashion but deploy it as microservices.
It's also possible to co-locate services on the same machine for more efficient
inter-service communication.
In that case, the services don't communicate via RPC but via normal Go function calls.
This allows developers to not have to think about splitting their app into microservices
beforehand, but gives them flexibility to adapt to new requirements and insights.
For example if two services communicate a lot with each other, it might make sense
to co-locate them on the same machine and scale them together.
Conversely, if a service is important for business needs it might make sense to
have this service as a separate microservice to increase fault-tolerance.
Service Weaver uses two techniques to enable these features:
A code generation step and a runtime that automatically tunes the performance
of the system.
The code generation generates efficient data and parameter serialization formats
that are consistent between services.
The runtime continually measures the performance and metrics of each service and
can then make educated decisions about the co-location of services, i.e.
move services that communicate with each other a lot to the same node.

Deploying a new version follows these steps:
1.) Make the necessary changes to the code.
2.) Run `weaver generate .` to let the code-generator update the infrastructure code.
3.) Deploy the new version with `weaver gke deploy weaver.toml`.

Service Weaver takes care of everything related to setting up the Kubernetes cluster,
Dockerfiles, networking, etc.
Configuring the Service Weaver application deployment is done via a weaver.toml file.
You can configure to which regions the application should be deployed, how fast the
deployment should take place, on what URLs your application should be available from,
and which components (if any) should be co-located.
Co-location here means, that they run on the same node and communicate via normal
Go function calls, thus decreasing the communication overhead even further.

\subsection{Traditional Microservice}

\Citeauthor{building_microservices} defines microservices as encapsulated parts
of a greater system that are accessible to each other via the network.
They also mention that microservices are independently releaseable and usually
don't share a database.
A typical MSA application consists of multiple folders for each microservice
that don't share code with each other.

To create a typical microservice I created two separate services in different folders.
The services are both written in Go and communicate with each other via REST endpoints and JSON messages.

\section{Evaluation}

I implemented the same service twice: As a Service Weaver application and as a typical
microservice based application.
While the code isn't identical I kept differences to a minimum as to not interfere
with performance measurements.
The exact research questions and metrics I used are explained in the next section,
followed by the experiment setup, and the results.

\subsection{Metrics and Research Questions}
Based on the claims of the Service Weaver framework, the following questions
present themselves:

\textbf{RQ1:} How much easier is it to split a microservice into two in a Service Weaver
application vs a traditional microservice app?
Imagine you have an existing service that you want to split up for whatever
reason.
It could be that you notice that the service is doing too many things
and you want to split it to contain the logic better.
Or maybe one part of the service is called more frequently than the other
and you want to scale them independently.
In that case you need to split an existing microservice into two separate
instances.

I measure easier/harder in this context by the amount of LOC changed, as well
as the places where those changes happened.
This specifically also includes configuration files (such as Helm charts or Dockerfiles).
I will also give a subjective rating to the different places where code
or configuration had to be changed: A change in a deployment pipeline
or Helm chart is potentially more error-prone or dangerous than a simple
change in a function.

\textbf{RQ2:} How much more efficient is the over-the-wire communication between two Service Weaver
microservices compared to two traditional microservices?
Service Weaver uses a custom serialization format to pass messages between services
that is more compact than JSON.

Efficiency will be measured in three ways: Amount of bytes transferred over
the network, the amount of time spent doing unnecessary work such as
JSON payload encoding and decoding, and the timespan it takes the
service to receive the request and return a response.

\subsection{Measurement Setup}

To reduce variance I conducted the measurements on two hosted VMs from Linode \cite{Linode}.
The first VM hosts the app and the second VM makes the requests and records the
metrics.
The exact VM specifications can be found in table \ref{tab:linode}.

Both apps consisted of two services: a main service and a worker service.
The main service receives requests and delegates them to the worker service.
The worker service receives requests from the main service, increments a number in the object
and returns the modified request object.
Then finally the main service returns the response to the client.
Note that the test cases do not include a call to a database or any kind of storage.
I just want to test the capabilities of the framework and not stain the results by
waiting on a data store.
The Service Weaver app was started using the `weaver gke-local` command which reproduces a
cloud deployment and is similar to a typical microservice deployment with each service running in
its own container/on its own node.
That means the services are communicating via RPC.
Because `weaver gke-local` creates two instances of each component and a load-balancing
proxy by default, I also replicated each service for the traditional app twice.
I also added a Caddy \cite{serverCaddyUltimateServer} reverse proxy to act a load-balancer.
The general architecture can be seen in \ref{fig:architecture}.

For each test no other services besides the app itself and the pre-installed system services were running.
I used the load-generation tool Apache Bench (\emph{ab}) \cite{ApacheHTTPServer} to simulate
real-world stress test.


\begin{table}
	\caption{Specifications of virtual machine used for testing.}
	\label{tab:linode}
	\begin{tabular}{ccc}
		\toprule
		CPU model & AMD EPYC 7601    & AMD EPYC 7642    \\
		CPU cores & 2                & 1                \\
		CPU speed & 2.199GHz         & 2.299GHz         \\
		RAM       & 4 GB             & 1 GB             \\
		OS        & Debian 11 64 bit & Debian 11 64 bit
	\end{tabular}
\end{table}


\begin{figure}
	\includegraphics[width=\columnwidth]{setup}
	\caption{The experimentation setup. The main service is called from the outside and
		forwards the request to the worker service.
		Afterwards the worker's response will be returned to the user.}
	\label{fig:architecture}
\end{figure}

\subsection{Results}

\subsubsection{How much more efficient is the custom serialization format?}

As seen in figure \ref{fig:performance1} the Service Weaver implementation performed
better than the traditional microservice architecture based application.
Some more numbers here...
This is likely due to the smaller footprint of the messages passed between the services.
Service Weaver uses a custom serialization format to passes function arguments between services.
This format stores the object as one long array in memory, binary encoded.
It works like this: integers are encoded as is and if no size is specified then
space for a 64-bit integer is reserved, 8 byte.
Strings are encoded by first storing the length of the string, followed by the string
itself in binary.
The JSON message passed between the tradional app's microservices is 549 byte big.
The keys have the following lengths: id - 2 byte, action - 6 byte, message - 7 byte.
The 10 quation marks take up 10 bytes of space in total.
That sums up to 25 bytes of unnecessary information that has to transmitted in every message.
One could say that the message has an overhead of 25 byte or 0.046 \%.
Comparing that to the Service Weaver encoding, we can see that it is smaller.
The Service Weaver encoding breaks down like so: 8 bytes for the id, 4 bytes for length
of the action string, 13 bytes for string "request\_image", 4 bytes for the length
of the message string, 500 bytes for the length of the message.
That adds up to a total of 8 + 4 + 13 + 4 + 500 = 529 bytes.
In this example the size savings aren't that big, but in different kinds of JSON messages
they can grow large.
The best case for the Service Weaver encoding is a JSON message with a lot of fields
with long names but short values.

\begin{figure}
	\includegraphics[width=\columnwidth]{serialization_code}
	\caption{Generated Service Weaver code for serializing objects
		as method arguments.}
	\label{fig:serialization_code}
\end{figure}

The shorter message size isn't the only contributing factor to the higher Service Weaver
performance though.
Service Weaver can also save time by not having to parse the JSON string before
creating a struct from the data.
In a typical microservice application the service receives the JSON request and
then has to first parse it into a struct of a given type.
During this parsing process it has to traverse the entire serialized JSON object
to make sure it is valid for the type of the struct.
Due to the code generation of Service Weaver it knows exactly how big each property
of the byte object is and can thus efficiently extract that information from the
byte array it receives and does not have to traverse the entire string.


In this case the Service Weaver code produced a message that is
Looking at the generated serialization code in figure \ref{fig:serialization_code}
we can see that the message is at least around

\subsubsection{Splitting Microservice}

\begin{figure}
	\includegraphics[width=\columnwidth]{minimal_component}
	\caption{A minimal Service Weaver component that exposes a function
		for adding two integers together.}
	\label{fig:minimal_component}
\end{figure}

In Service Weaver the concept of microservices is represented by components.
A component is an entity that can be replicated and placed wherever desired.
All communication happens through components.
To split such a microservice (or component) one would need to create a new
component that takes over some of the functionality of the old service.
This is done by creating a new public type and a private type that implements
that public type, typically in a new file as well.
A minimal component is shown in \ref{fig:minimal_component}.
Every component that now wants to use the new component just needs get a reference
to the new type like shown in \ref{fig:minimal_component}.
After that is done, each place in the code where the new component is to be used
needs to be updated.
Should we forgot any place, then the compiler will fail and display an error message
-- we can't build the application before fixing all those errors and references.
After that we can generate the new Service Weaver boilerplate code and then deploy
the new version.

\begin{table}[h]
	\centering
	\caption{Comparison of actions involving splitting a Service Weaver and a
		traditional microservice into two.}
	\label{tab:split_ms}
	\begin{tabularx}{\columnwidth}{cXX}
		\toprule
		Metric                     & Service Weaver                                                                   & Traditional Application                                                          \\
		\midrule
		Source files updated       & old microservice, new microservice, each file that references old or new service & old microservice, new microservice, each file that references old or new service \\
		Config files updated       & none                                                                             & new Dockerfile, new Helmfile, update deployment pipeline                         \\
		Errors caught by compiler? & Yes                                                                              & No                                                                               \\
		\bottomrule
	\end{tabularx}
\end{table}


% \begin{table}[h]
% 	\centering
% 	\begin{tabular}{lllll}
% 		\toprule
% 		\multirow{2}{*}{Models} & \multicolumn{3}{c}{Metric 1} & Metric 2                  \\
% 		\cmidrule{2-4} \cmidrule{5-5}                                                      \\
% 		{}                      & precision                    & recall   & F-score & R@10 \\
% 		\midrule
% 		model 1                 & 0.67                         & 0.8      & 0.729   & 0.75 \\
% 		model 2                 & 0.8                          & 0.9      & 0.847   & 0.85 \\
% 		\bottomrule
% 	\end{tabular}
% \end{table}

The traditional microservice based app consists of multiple microservices that each
have their own deployment configuration.
Each service has \textemdash in addition to its code \textemdash a Dockerfile, and a Helm
values file.
In order to split an existing microservice we first need to create a new folder
for the service and move all the code there.
After that we also need to add a Dockerfile and create a Helm values file and move
all the necessary configuration details there.
We also need to make sure to update our deployment pipeline to deploy the new
service.
Lastly, we have to go through each microservice and update them to call the new endpoint.
Here the programmer carries a huge burden, because the compiler can't help them.

See \ref{tab:split_ms} for a comparison.

\section{Discussion}
Service Weaver currently only supports GKE deployments natively.
No control over Dockerfiles?
No control over pod resources?
Tradeoff ease-of-use vs configuration.

One of the main benefits of developing a MSA application is the ability to
have large teams work on the same application and deploy idependently without
interfering with each other. \cite{what}
While this development model would certainly still be possible with a Service Weaver
application, it remains to be seen if the added cost and overhead of having atomic
deploys is worth the safety guarantees.

Another thing to note is that while Service Weaver does offer performance benefits
over typical JSON-based MSA applications, these benefits don't translate to 3rd party
services since they still need JSON messages to communicate.

A current drawback of Service Weaver is that its deployment is currently limited to Google's
cloud offering.
Google Cloud is only the third largest cloud provider behind AWS \cite{AmazonWebServices}
and Microsoft Azure \cite{Azure}.
If Service Weaver is to have a big impact they need to also support the leading
cloud providers.

\section{Conclusion}
This is the conclusion.

\bibliographystyle{ACM-Reference-Format}
\bibliography{service-weaver}
\end{document}
