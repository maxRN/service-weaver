\documentclass[sigconf,review,9pt]{acmart}
\graphicspath{ {./figures/} }
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{multirow}

\title{Service Weaver: A new era of developing microservice architectures?}
\author{Max Gro√üe}
\date{June 2023}

\begin{document}

\begin{abstract}
	Microservice architectures are quickly becoming the default architecture style
	in the industry, increasing development velocity and reducing scaling issues.
	However numerous problems have been identified that plague microservice development,
	such as designing the overall architecture and high network usage.
	Service Weaver is a new framework that aims to solve some of the challenges involved
	in microservice development.
	In this paper I find that Service Weaver shows up to a 10\% reduction in message size
	and thus faster communication speeds.
	It also simplifies operating large applications by providing integration with
	logging, metrics, and trace tooling of cloud providers.
	It remains to be seen whether the simplified configuration of Service Weaver application
	can cover all use cases.
\end{abstract}

\maketitle

\section{Introduction}
Microservices are the predominant way to build modern applications in the industry. \cite{wang_ms_current_situation}
The advantages over the typical monolithic development have been numerous:
better horizontal scalability due to individually deployable and scalable entities,
better suited for large teams due to independence of each other's implementation,
and more agile development.

However, recent papers have shown that microservice architectures bring their own share
of problems that developers have to deal with.
In \cite{soldani_pains_gains} the authors analyze the pains for MSA development
and create a taxonomy for each stage of the lifecycle of software (design, development, and operation).
Architecture design, testing during development, and monitoring and resource consumption
while operating the application are among the most often talked about pains.

Service Weaver is a new development framework created by Google that aims to solve some of
the problems with current microservice development.
Their solution offers the flexibility to write the application as a typical monolith
but deploy it as a set of microservices, thus eliminating the problem of pre-mature
splitting of the application.

Based on the pain taxonomy presented in \cite{soldani_pains_gains} I am going to
compare a traditional microservice application written in Go with a Service Weaver
implementation.
\Citeauthor{soldani_pains_gains} identify multiple pain points, but I'm going to focus
on just a few for each stage of the development lifecycle:
\begin{itemize}
	\item{Design: service dimensioning, API versioning, service contracts}
	\item{Development: Amount of boilerplate code for communication}
	\item{Operations: Network resource usage}
\end{itemize}

\subsection{Related Work}
The team behind Service Weaver present the programming model and architecture
as well as a few performance measurements in \cite{service_weaver_paper}.
While they go in-depth on the technical details of the architecture, they don't
specify how exactly the tests were performed.

\Citeauthor{soldani_pains_gains} conduct a grey literature review of pains and gains
associated with microservice development.
They present a pain and a gain taxonomy of a typical microservice based application
development lifecycle.
They don't however present any solution to relieve these problems.

Instead of JSON-based messaging a binary message format such as gRPC \cite{GRPC}
or Apache Avro \cite{ApacheAvro} can be used to increase performance.
This approach comes with its own challenges however: The message definitions have
to be kept in sync between services and there is still the possibility of services
with different message versions running in parallel.

As of todate, there haven't been any 3rd party performance or capability tests
of the Service Weaver framework.
That is understandable as this framework is still rather new with it's first
open-source version having been released in March 2023.
While the team behind Service Weaver has published a paper about the internals
which also included some performance measurements, they are far from complete
and omit crucial information.
This paper aims to solve this gap in reaserch by providing detailed performance
analysis.

\section{Fundamentals}

To effectively measure the differences in performance and to understand the results
it is important to understand the different architectures and technical details.

\subsection{Service Weaver}
Service Weaver is a new framework developed by Google for creating Microservice
Architectures in Go. \cite{serviceWeaverSite}
It enables the developer to write their app in a monolithic fashion but deploy it as microservices.
It's also possible to co-locate services on the same machine for more efficient
inter-service communication.
In that case, the services don't communicate via RPC but via normal Go function calls.
This allows developers to not have to think about splitting their app into microservices
beforehand, but gives them flexibility to adapt to new requirements and insights.
For example if two services communicate a lot with each other, it might make sense
to co-locate them on the same machine and scale them together.
Conversely, if a service is important for business needs it might make sense to
have this service as a separate microservice to increase fault-tolerance.
Service Weaver uses two techniques to enable these features:
A code generation step and a runtime that automatically tunes the performance
of the system.
The code generation generates efficient data and parameter serialization formats
that are consistent between services.
The runtime continually measures the performance and metrics of each service and
can then make educated decisions about the co-location of services, i.e.
move services that communicate with each other a lot to the same node.

Deploying a new version follows these steps:
1.) Make the necessary changes to the code.
2.) Run `weaver generate .` to let the code-generator update the infrastructure code.
3.) Deploy the new version with `weaver gke deploy weaver.toml`.

Service Weaver takes care of everything related to setting up the Kubernetes cluster,
Dockerfiles, networking, etc.
Configuring the Service Weaver application deployment is done via a weaver.toml file.
You can configure to which regions the application should be deployed, how fast the
deployment should take place, on what URLs your application should be available from,
and which components (if any) should be co-located.
Co-location here means, that they run on the same node and communicate via normal
Go function calls, thus decreasing the communication overhead even further.

\subsubsection{Custom Serialization Format}

Service Weaver uses a custom serialization format to improve microservice communication
efficiency and performance.
During its code generation step Service Weaver creates a serialization code for the
parameters of a method of a microservice that is called from another microservice and
also generates deserialization code of the return parameters.
Analogously, for the called on microservice it generates deserialization code for
the parameters and serialization code for the return parameters.
Due to knowing all dependencies at compile time it can generate this code for all
microservices.

The protocol works as follows: Each integer is encoded as is, a string is encoded
as the length of the string followed by the contents of the string.
This encoding has two advantages: First the overall payload is smaller than the
JSON payload for the same information, because the field names aren't transmitted
for every request. Second, because Service Weaver knows the exact order of the arguments,
it doesn't need to parse the request payload: it can just extract the values directly
from the byte slice.
An example code snippet for generated serialization code is given in figure \ref{fig:serialization_code}.

\begin{figure}
	\includegraphics[width=\columnwidth]{serialization_code}
	\caption{Generated Service Weaver code for serializing objects
		as method arguments.}
	\label{fig:serialization_code}
\end{figure}

\subsection{Traditional Microservice}

\Citeauthor{building_microservices} defines microservices as encapsulated parts
of a greater system that are accessible to each other via the network.
They also mention that microservices are independently releaseable and usually
don't share a database.
A typical MSA application consists of multiple folders for each microservice
that don't share code with each other.

To create a typical microservice I created two separate services in different folders.
The services are both written in Go and communicate with each other via REST endpoints
and JSON messages.
JSON-based communication is typical for services that are not performance-critical.
As JSON is a human- and machine-readable format it suits itself nicely for develoment,
while still being more efficient than XML-based communication protocols.

\section{Evaluation}

I implemented the same service twice: As a Service Weaver application and as a typical
microservice based application.
While the code isn't identical I kept differences to a minimum as to not interfere
with performance measurements.
The exact research questions and metrics I used are explained in the next section,
followed by the experiment setup, and the results.

\subsection{Metrics and Research Questions}
Based on the claims of the Service Weaver framework, the following questions
present themselves:

\textbf{RQ1:} How much easier is it to split a microservice into two in a Service Weaver
application vs a traditional microservice app?
Imagine you have an existing service that you want to split up for whatever
reason.
It could be that you notice that the service is doing too many things
and you want to split it to contain the logic better.
Or maybe one part of the service is called more frequently than the other
and you want to scale them independently.
In that case you need to split an existing microservice into two separate
instances.

I measure easier/harder in this context by the amount of LOC changed, as well
as the places where those changes happened.
This specifically also includes configuration files (such as Helm charts or Dockerfiles).
I will also give a subjective rating to the different places where code
or configuration had to be changed: A change in a deployment pipeline
or Helm chart is potentially more error-prone or dangerous than a simple
change in a function.

\textbf{RQ2:} How much more efficient is the over-the-wire communication between two Service Weaver
microservices compared to two traditional microservices?
Service Weaver uses a custom serialization format to pass messages between services
that is more compact than JSON.

Efficiency will be measured in three ways: Amount of bytes transferred over
the network, the amount of time spent doing unnecessary work such as
JSON payload encoding and decoding, and the timespan it takes the
service to receive the request and return a response.

\subsection{Measurement Setup}

In this section I will firstly explain the general experimentation setup that is
used to evaluate the performance of both applications.
Then I will go on to explain the different scenarios I considered in the evaluation.
Lastly I cover the tools and hardware used to conduct the measurements.

Both apps consist of two services: a main service and a worker service.
The main service receives requests and delegates them to the worker service.
The worker service receives requests from the main service, increments a number in the payload
object and returns the modified request object.
Then finally the main service returns the response to the client.
Note that this setup does not include any kind of database calls as this would slow
down the request and it not relevant to the performance of each application.
The Service Weaver app was started using the `weaver gke-local` command which reproduces a
cloud deployment and is similar to a typical microservice deployment with each service running in
its own container/on its own node.
This also ensures that the services are communicating via RPC, because usually Service Weaver
components on the same machine would communicate using normal Go method calls.
Because `weaver gke-local` creates two instances of each component and a load-balancing
proxy by default, I also replicated this setup for the traditional microservice application.
Thus for the traditional microservice I started each service (main and worker) twice
and additionally put a reverse-proxy load-balancer \cite{serverCaddyUltimateServer}
in front of the main services to distribute the load evenly.
The general architecture can be seen in \ref{fig:architecture}.

I evaluated the performance of the services by sending payloads of varying sizes
at different network load levels.
There are 3 scenarios: A) no-payload, B) "wide" JSON payload, C) "narrow" JSON payload.
Wide and narrow refers to the amount of fields the payload has vs. the size of the
value of each field.
The wide payload has fewer fields, but larger values while the narrow payload
has more fields that each hold only a small amount of information.
The payloads were chosen that way to measure the impact of the custom Service
Weaver encoding.
Both payload types consist of an id and an action field.
The narrow JSON payload has an additional 20 fields with values of size 10 bytes.
The wide JSON payload has only one additional field with a value of size 500 bytes.
The no-payload scenario is used to establish a baseline comparison and to track
any slow downs the reverse proxy (Caddy) used in the traditional application
might incur.

To reduce variance I conducted the measurements on two hosted VMs from Linode \cite{Linode}.
The first, more powerful VM hosts the app while the second, less powerful VM makes
the requests and records the metrics.
The exact VM specifications can be found in table \ref{tab:linode}.
For each test no other services besides the app itself and the pre-installed system services were running.
I used the load-generation tool Apache Bench (\emph{ab}) \cite{ApacheHTTPServer} to simulate
real-world stress test.
I varied the amount of requests as well as the concurrency level of the requests with
the flags n for number of request and c for concurrency level.

\begin{table}
	\caption{Specifications of virtual machine used for testing.}
	\label{tab:linode}
	\begin{tabular}{ccc}
		\toprule
		CPU model & AMD EPYC 7601    & AMD EPYC 7642    \\
		CPU cores & 2                & 1                \\
		CPU speed & 2.199GHz         & 2.299GHz         \\
		RAM       & 4 GB             & 1 GB             \\
		OS        & Debian 11 64 bit & Debian 11 64 bit \\
		\bottomrule
	\end{tabular}
\end{table}


\begin{figure}
	\includegraphics[width=\columnwidth]{setup.excalidraw}
	\caption{The experimentation setup. The main service is called from the outside and
		forwards the request to the worker service.
		Afterwards the worker's response will be returned to the user. LB = load-balancer.
		Each service is replicated twice.}
	\label{fig:architecture}
\end{figure}

\subsection{Results}

\subsubsection{RQ1 Performance Evaluation}

\begin{figure}
	\includegraphics[width=\columnwidth]{nothing_performance.png}
	\caption{Nothing scenario performance for traditional and Service Weaver application.}
	\label{fig:nothing_performance}
\end{figure}

\begin{figure}
	\includegraphics[width=\columnwidth]{wide_performance.png}
	\caption{Wide scenario performance for traditional and Service Weaver application.}
	\label{fig:wide_performance}
\end{figure}

\begin{figure}
	\includegraphics[width=\columnwidth]{long_performance.png}
	\caption{Long scenario performance for traditional and Service Weaver application.}
	\label{fig:long_performance}
\end{figure}

Figure \ref{fig:nothing_performance} shows the base line performance measurements
when not sending any JSON payload and just communicating with the main service.
Both are relatively close in performance, but the traditional microservice application
reliably outperforms the Service Weaver application.
That is likely due to the automatically generated traces from Service Weaver.
This allows easier debugging, but also slows down the application.

Figure \ref{fig:wide_performance} displays the performance when sending a JSON object
with one field with a 500 byte long value.
The size of the JSON request payload transmitted via HTTP in the traditional microservice
application is 547 bytes.
The binary encoded Service Weaver message is only 531 bytes, which is only around
0.03\% smaller.
Yet, according to figure \reg{fig:wide_performance} the Service Weaver application
performs up to 19\% or 44 seconds faster than the traditional application in the 50k requests scenario.
This can be attributed to


% 8 bytes (for ID) +
% string:
%   4 bytes (length) + 15 bytes message
% string:
%   4 bytes (length) + 500 bytes message
% = 531 bytes


Figure \ref{fig:long_performance} shows the comparison between Service Weaver and
the traditional application when sending a "long" JSON message.
That is a JSON object that has a lot of small fields, which means the Service Weaver
encoding should reduce the size by a lot compared to the JSON messages sent between
the traditional application's microservices.


As seen in figure \ref{fig:performance1} the Service Weaver implementation performed
better than the traditional microservice architecture based application.
Some more numbers here..
This is likely due to the smaller footprint of the messages passed between the services.
Service Weaver uses a custom serialization format to passes function arguments between services.
This format stores the object as one long array in memory, binary encoded.
It works like this: integers are encoded as is and if no size is specified then
space for a 64-bit integer is reserved, 8 byte.
Strings are encoded by first storing the length of the string, followed by the string
itself in binary.
The JSON message passed between the tradional app's microservices is 549 byte big.
The keys have the following lengths: id - 2 byte, action - 6 byte, message - 7 byte.
The 10 quation marks take up 10 bytes of space in total.
That sums up to 25 bytes of unnecessary information that has to transmitted in every message.
One could say that the message has an overhead of 25 byte or 0.046 \%.
Comparing that to the Service Weaver encoding, we can see that it is smaller.
The Service Weaver encoding breaks down like so: 8 bytes for the id, 4 bytes for length
of the action string, 13 bytes for string "request\_image", 4 bytes for the length
of the message string, 500 bytes for the length of the message.
That adds up to a total of 8 + 4 + 13 + 4 + 500 = 529 bytes.
In this example the size savings aren't that big, but in different kinds of JSON messages
they can grow large.
The best case for the Service Weaver encoding is a JSON message with a lot of fields
with long names but short values.

The shorter message size isn't the only contributing factor to the higher Service Weaver
performance though.
Service Weaver can also save time by not having to parse the JSON string before
creating a struct from the data.
In a typical microservice application the service receives the JSON request and
then has to first parse it into a struct of a given type.
During this parsing process it has to traverse the entire serialized JSON object
to make sure it is valid for the type of the struct.
Due to the code generation of Service Weaver it knows exactly how big each property
of the byte object is and can thus efficiently extract that information from the
byte array it receives and does not have to traverse the entire string.


In this case the Service Weaver code produced a message that is
Looking at the generated serialization code in figure \ref{fig:serialization_code}
we can see that the message is at least around

\subsubsection{Splitting Microservice}

\begin{figure}
	\includegraphics[width=\columnwidth]{minimal_component}
	\caption{A minimal Service Weaver component that exposes a function
		for adding two integers together.}
	\label{fig:minimal_component}
\end{figure}

In Service Weaver the concept of microservices is represented by components.
A component is an entity that can be replicated and placed wherever desired.
All communication happens through components.
To split such a microservice (or component) one would need to create a new
component that takes over some of the functionality of the old service.
This is done by creating a new public type and a private type that implements
that public type, typically in a new file as well.
A minimal component is shown in figure \ref{fig:minimal_component}.
Every component that now wants to use the new component just needs get a reference
to the new type like shown in figure \ref{fig:minimal_component}.
After that is done, each place in the code where the new component is to be used
needs to be updated.
Should we forgot any place, then the compiler will fail and display an error message
-- we can't build the application before fixing all those errors and references.
After that we can generate the new Service Weaver boilerplate code and then deploy
the new version.

\begin{table}[h]
	\centering
	\caption{Comparison of actions involving splitting a Service Weaver and a
		traditional microservice into two.}
	\label{tab:split_ms}
	\begin{tabularx}{\columnwidth}{cXX}
		\toprule
		Metric                     & Service Weaver                                                                   & Traditional Application                                                          \\
		\midrule
		Source files updated       & old microservice, new microservice, each file that references old or new service & old microservice, new microservice, each file that references old or new service \\
		Config files updated       & none                                                                             & new Dockerfile, new Helmfile, update deployment pipeline                         \\
		Errors caught by compiler? & Yes                                                                              & No                                                                               \\
		\bottomrule
	\end{tabularx}
\end{table}


% \begin{table}[h]
% 	\centering
% 	\begin{tabular}{lllll}
% 		\toprule
% 		\multirow{2}{*}{Models} & \multicolumn{3}{c}{Metric 1} & Metric 2                  \\
% 		\cmidrule{2-4} \cmidrule{5-5}                                                      \\
% 		{}                      & precision                    & recall   & F-score & R@10 \\
% 		\midrule
% 		model 1                 & 0.67                         & 0.8      & 0.729   & 0.75 \\
% 		model 2                 & 0.8                          & 0.9      & 0.847   & 0.85 \\
% 		\bottomrule
% 	\end{tabular}
% \end{table}

The traditional microservice based app consists of multiple microservices that each
have their own deployment configuration.
Each service has \textemdash in addition to its code \textemdash a Dockerfile, and a Helm
values file.
In order to split an existing microservice we first need to create a new folder
for the service and move all the code there.
After that we also need to add a Dockerfile and create a Helm values file and move
all the necessary configuration details there.
We also need to make sure to update our deployment pipeline to deploy the new
service.
Lastly, we have to go through each microservice and update them to call the new endpoint.
Here the programmer carries a huge burden, because the compiler can't help them.

See \ref{tab:split_ms} for a comparison.

\section{Discussion}
% TODO:
% Service Weaver currently only supports GKE deployments natively.
% No control over Dockerfiles?
% No control over pod resources?
% Tradeoff ease-of-use vs configuration.

One of the main benefits of developing a MSA application is the ability to
have large teams work on the same application and deploy idependently without
interfering with each other. \cite{what}
While this development model would certainly still be possible with a Service Weaver
application, it remains to be seen if the added cost and overhead of having atomic
deploys is worth the safety guarantees.

Another thing to note is that while Service Weaver does offer performance benefits
over typical JSON-based MSA applications, these benefits don't translate to 3rd party
services since they still need JSON messages to communicate.

One of the benefits of microservice based development in large organizations with
lots of teams is the ability for each team to choose their own technology stack.
In the case of Service Weaver the language is pre-defined: Go.

A current drawback of Service Weaver is that its deployment is currently limited to Google's
cloud offering.
According to \cite{InfographicBigThree2023} AWS \cite{AmazonWebServices} has a
market share of 32\% and Azure \cite{Azure} of 23\% respectively.
Meaning that both are ahead of Google's cloud offering \cite{GoogleCloud} in market
share at just 10\%.
If Service Weaver is to have a big impact they need to also support the leading
cloud providers.
However, due to Service Weaver's design it should not be too difficult to implement deployer
modules for the other two leading cloud providers as the deployer module is only
loosely coupled to the rest of the Service Weaver architecture. \cite{HowImplementService}

\section{Conclusion}
In this paper I conducted a performance analysis of a Service Weaver application
and a typical microservice architecture application written in Go and communicating
with JSON messages over REST endpoints.
The experiments have shown that Service Weaver has a higher throughput in communication
between services due to its efficient custom binary serilization format.

Teams that run their infrastructure on Google's cloud and are already developing
their services in Go should consider using Service Weaver for their next project
as its improved developer experience and more efficient network communication can
result in cost savings.

\bibliographystyle{ACM-Reference-Format}
\bibliography{service-weaver}
\end{document}
